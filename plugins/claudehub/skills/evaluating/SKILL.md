---
name: evaluating
description: This skill should be used when assessing artifacts, outputs, or content across multiple dimensions - provides frameworks for dimension selection, parallel evaluation, and concrete upgrade paths to achieve 10/10 quality.
version: 1.0.0
---

# Evaluating Artifacts

Assess output quality across contextually relevant dimensions, with concrete paths to excellence.

## Principles

- **Context-driven dimensions** - Select 5-7 dimensions based on the artifact type and purpose. No fixed rubric.
- **Target excellence** - 10/10 is the standard. Anything below 9 requires specific, actionable upgrades.
- **Parallel evaluation** - Assess each dimension independently to avoid bias bleeding between scores.
- **Surgical revision** - When upgrading, preserve voice and structure. Only fix what's weak.

## Dimension Selection

Propose dimensions based on what matters for *this specific artifact*. Consider:
- Artifact type (code, documentation, design, communication)
- Intended audience
- Success criteria
- Common failure modes

Get user confirmation before evaluating. They may have context on what matters most.

## Evaluation Format

For each dimension:

### [Dimension Name] â€” X/10
**Why this dimension**: 1-2 sentences on relevance
**What's working**: Specific strengths observed
**What's missing**: Specific gaps identified
**Upgrade to 10/10**: Concrete actions to close gaps

## Revision Guidelines

When creating a 10/10 version:
- Preserve original structure, voice, and core content
- Only upgrade what's necessary for weak dimensions
- Default to surgical edits, not rewrites
- Show changes clearly so the user can learn
